<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>PROJECTS on h4pZ</title>
    <link>http://localhost:1313/projects/</link>
    <description>Recent content in PROJECTS on h4pZ</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 10 Sep 2023 20:53:50 +0200</lastBuildDate>
    <atom:link href="http://localhost:1313/projects/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Dota 2 Toxicity</title>
      <link>http://localhost:1313/projects/dota2/</link>
      <pubDate>Sun, 10 Sep 2023 20:53:50 +0200</pubDate>
      <guid>http://localhost:1313/projects/dota2/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;&#xA;&lt;p&gt;Dota 2 is a multiplayer online battle arena (MOBA) game. It’s one of the most popular competitive games in the market and has the biggest prize pool in a single event of all e-sports [13]. Due to its high popularity and e-sport scene [2], Dota 2 is regarded as a highly competitive game. As a Dota 2 player myself, I’ve experienced harassment and toxic behaviour in-game. This is not however, an isolated experience. In fact, Dota 2 is regarded as the most toxic online competitive game [2]. Harassment and toxicity can be generated by three means. The first one is through griefing, which involves using in-game mechanics in order to downgrade the playing experience of other players. The second channel where it can happen is in the in-game voice chat. Finally, the last channel where it happens is the in-game text channel, where players can freely communicate with their teammates or enemies.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Variational Autoencoder</title>
      <link>http://localhost:1313/projects/vae/</link>
      <pubDate>Sun, 10 Sep 2023 20:50:45 +0200</pubDate>
      <guid>http://localhost:1313/projects/vae/</guid>
      <description>&lt;p&gt;WIP&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../images/projects/vae/cvae_interpolation.gif&#34; alt=&#34;img&#34;&gt;&#xA;&lt;img src=&#34;../images/projects/vae/cvae_random.gif&#34; alt=&#34;img&#34;&gt;&#xA;&lt;img src=&#34;../images/projects/vae/cvae_train.gif&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Image to Image translation</title>
      <link>http://localhost:1313/projects/img2img/</link>
      <pubDate>Sun, 10 Sep 2023 20:38:20 +0200</pubDate>
      <guid>http://localhost:1313/projects/img2img/</guid>
      <description>&lt;p&gt;Image to image translation is part of what is known as cross-domain knowledge. The idea behind cross-domain knowledge is that there is a shared representation of an object in two domain spaces. For example, a photograph of persons face have a representation as a black and white sketch and vice versa. In this exploration I use the cyclegan model, which uses two autoencoders that translate one image from one domain to another, and two discriminators that judge whether an image was sampled from the actual domain or was generated. In this case I wanted to find a common representation between sculpture faces and actual faces. I got some interesting results from faces to sculptures, however for the translation between sculptures and faces, I was left with why I called a &amp;ldquo;vaporwave&amp;rdquo; filter.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mixture Density Network</title>
      <link>http://localhost:1313/projects/mixture-density-network/</link>
      <pubDate>Sun, 10 Sep 2023 20:26:47 +0200</pubDate>
      <guid>http://localhost:1313/projects/mixture-density-network/</guid>
      <description>&lt;p&gt;The mixture density network or MDN was specified by Christopher Bishop in his book Pattern Recognition and Machine Learning. This model tries to predict the marginal probability of a variable Y by a mixture of Gaussian distributions and a latent variable z defined by the network it self. One advantage of this architecture, unlike the softmax function, is that it can express a wide range of probability distributions. For this project I implemented a multivariate Gaussian density network with the purpose to predict (like in the painting images with neural networks post) the most probable (R, G, B) vector  for a pixel wich position is given by a point (x, y) in R2. I trained this network with two images. One oil painting by Henrik Uldalen and an illustration by Simon Weaner.&#xA;View fullsize Simon Weaner Illustration&#xA;Simon Weaner Illustration&#xA;View fullsize Henrik Uldalen oil painting&#xA;Henrik Uldalen oil painting&lt;/p&gt;</description>
    </item>
    <item>
      <title>Draw</title>
      <link>http://localhost:1313/projects/draw/</link>
      <pubDate>Sun, 10 Sep 2023 20:16:03 +0200</pubDate>
      <guid>http://localhost:1313/projects/draw/</guid>
      <description>&lt;p&gt;Draw is a model that recreates the mechanism of attention done by humans. This model has T time steps (defined by the user) over which the recurrent neural network will recreate the original image step by step. Letting the network decide which patches of the image is going to draw first in a blank canvas. I used this model over a single image instead of over a whole data set, because I wanted to see how the model which is intended to work with MNIST, or CIFAR would work over one picture. I trained the DRAW network with two images, the first one which I found randomly on archillet project and the other one which is done by Laura Makabresku (NSFW warning for the LM webpage).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deep Dream</title>
      <link>http://localhost:1313/projects/deep-dream/</link>
      <pubDate>Sun, 10 Sep 2023 19:37:25 +0200</pubDate>
      <guid>http://localhost:1313/projects/deep-dream/</guid>
      <description>&lt;p&gt;The objective of this project/course work for CADL was to explore two styling techniques using deep learning. The first one is related with guided dreams/hallucinations, leveraging on the original idea of Inceptionism. The second one is based on style transfer. Both of these ideas were executed using the pre-trained network Inception (v5) of Google and using two paintings by Kim-Byngkwan and one picture done by Carol Pinzon. The subjects for this experiment are presented below.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Painting With Neural Networks</title>
      <link>http://localhost:1313/projects/painting-with-neural-networks/</link>
      <pubDate>Sun, 10 Sep 2023 19:25:34 +0200</pubDate>
      <guid>http://localhost:1313/projects/painting-with-neural-networks/</guid>
      <description>&lt;p&gt;This project was born as one of my first course work for the  Creative Applications of Deep Learning  course on Kadenze (which I recommend). The idea of this project was to train a function that maps points (x, y) regarding the location of a pixel in R2, to a point (R, G, B) corresponding to the values of each channel in the pixel.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
