<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="../../css/styles.css">
<link rel="stylesheet" href="../../css/navbar.css">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap" rel="stylesheet">
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true
    }
  });
</script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

</head>
<body>
    <header>
        <nav id="navbar">
    <div class="articles">
        <article class="section">
            <p><a href="../../" >H4PZ</a></p>
        </article>
        <article class="section">
            <p><a href="../../projects" >PROJECTS</a></p>
        </article>
        <article class="section">
            <p><a href="../../blog" >BLOG</a></p>
        </article>
        <article class="section">
            <p><a href="../../about" >ABOUT</a></p>
        </article>
    </div>
</nav>


    </header>
    
    
<div class="content">
  <div class="meta-data-container">Sunday, Sep 10, 2023</div>
  <div ><h3>Expectation Maximization Algorithm</h3></div>
  <div ><p>An expectation maximization exmaple</p>
<hr>
<p>What this means is that there exists a data generator process to which we can only collect (or it was only collected) part of the data $X$ associated with it, instead of the full data $X, Z$. Usually a model is constructed around the marginalized maximum likelihood estimator:</p>
<p>$$\underset{\theta}{\mathrm{argmax}} \text{ } P\left(X | \theta\right) = \underset{\theta}{\mathrm{argmax}} \int P\left(X, Z | \theta\right) dZ$$</p>
<p>$$\text{Where,}$$</p>
<p>$$X: \text{observed variables}$$</p>
<p>$$Z: \text{latent variables (non observed data)}$$
However, sometimes maximizing the marginalized likelihood directly is hard. This is where the EM (Expectation Maximization) algorithm comes into play. Instead of maximizing over the marginalized likelihood, the optimization is made over the complete data likelihood.</p>
<p>$$\underset{\theta}{\mathrm{argmax}} \text{ } P\left(X, Z | \theta\right)$$</p>
<p>This procedure is also going to optimize the marginalized likelihood, but before going into why and how it works, let’s introduce a toy problem to get a better grasp. Imagine a binary data set $X$ (coin flips if you wish) generated by $K$ different multivariate Bernoulli distributions (only one distribution generates a given data point).</p>
<p>$$P\left(X| Z, \mu \right) = \prod_{k=1}^{K} P\left(X | \mu_k \right)^{z_k} = \prod_{k=1}^{K}  \prod_{n=1}^{N}  \prod_{i=1}^{D} \left[\mu_{k, i}^{x_{n,i}} (1 -\mu_{k, i})^{(1 - x_{n,i})}\right]^{z_k}$$</p>
<p>$$\text{Where,}$$</p>
<p>$$z_k: \text{binary variable that indicates if the Bernoulli distribution k generated the observation n}$$</p>
<p>Unfortunately by definition we don’t have access to the variables $Z$ since it wasn’t observed. Which means that we need to construct a different model. For this particular example, the model which is being used is a mixture of multivariate Bernoulli distributions.</p>
<p>$$P\left(X| \mu, \pi \right) = \prod_{n=1}^{N}  \sum_k^K \pi_k P\left(x_n| \mu_k\right)$$</p>
<p>$$\text{Where,}$$</p>
<p>$$\pi_k: \text{probability that the Bernoulli distribution k generated the observation n}$$</p>
<p>Trying to maximize the log-likelihood will not yield any result since it’s intractable. It can be seen below given the summation inside the logarithm. So, in order to solve this optimization problem, the EM algorithm is used. This is an iterative process divided in two steps. The first one is called the expectation step and the second one is called the maximization step.</p>
<p>$$\log P\left(X| \mu, \pi \right) = \sum_{n=1}^{N} \log \left[\sum_{k=1}^{K} \pi_k P\left(x_n | \mu_k \right) \right]$$</p>
<p>Expectation step</p>
<p>In this first step, the expectation over the complete data log-likelihood is computed.</p>
<p>$$\mathbb{E}<em>{Z | X, \mu, \pi} \left[\log \mathcal{L}\left(\mu, \pi ; X, Z \right)\right] = \mathbb{E}</em>{Z | X, \mu, \pi} \left[\log P\left(X, Z | \mu, \pi\right) \right]$$</p>
<p>$$=\mathbb{E} \left[\log P\left(X, Z | \mu, \pi\right) \right] = \mathbb{E} \left[ \sum_{n=1}^{N} \sum_{k=1}^{K} \log \left[\pi_k P\left(x_n | \mu_k \right) \right]^{z_{n,k}} \right] $$</p>
<p>$$= \mathbb{E} \left[ \sum_{n=1}^{N} \sum_{k=1}^{K} z_{n,k} \left[\log \pi_k + \log P\left(x_n | \mu_k \right) \right] \right] = \mathbb{E} \left[ \sum_{n=1}^{N} \sum_{k=1}^{K} z_{n,k} \left[\log \pi_k + \log  \prod_{i=1}^{D} \left[\mu_{k, i}^{x_{n,i}} (1 -\mu_{k, i})^{(1 - x_{n,i})}\right] \right] \right]$$</p>
<p>$$= \mathbb{E} \left[ \sum_{n=1}^{N} \sum_{k=1}^{K} z_{n,k} \left[\log \pi_k + \sum_{i=1}^{D} x_{n,i} \log \mu_{k, i} + \left(1-x_{n,i}\right) \log \left(1 - \mu_{k,i}\right)\right] \right]$$</p>
<p>$$= \sum_{n=1}^{N} \sum_{k=1}^{K} \mathbb{E} \left[z_{n,k}\right] \left[\log \pi_k + \sum_{i=1}^{D} x_{n,i} \log \mu_{k, i} + \left(1-x_{n,i}\right) \log \left(1 - \mu_{k,i}\right)\right] $$</p>
<p>In the expression above $\mathbb{E} \left[z_{n,k}\right]$ is known as the resposibility $\gamma \left(z_{n,k}\right)$ or the posterior probability $P\left(Z | X, \mu, \pi \right)$.</p>
<p>Maximization step</p>
<p>In this step the expected log-likelihood shown above is maximized with respect to the model parameters. The optimization problem is described below:</p>
<p>$$\underset{\mu, \pi}{\mathrm{argmax}} \text{ } \mathbb{E}_{Z | X, \mu, \pi} \left[\log \mathcal{L}\left(\mu, \pi ; X, Z \right)\right] $$</p>
<p>$$s.t.$$</p>
<p>$$\sum_{k=1}^{K} \pi_k = 1$$</p>
<p>The solution to this problem yields the following equations for the model parameters $\pi$ and $\mu$.</p>
<p>$$\pi^{EML} = \left[\frac{\sum_{N}P\left(Z | X, \mu, \pi \right)}{N}\right]_k$$</p>
<p>$$\mu^{EML} = \left[\frac{\sum_{N}x_{ni}P\left(Z | X, \mu, \pi \right)}{\sum_{N}P\left(Z | X, \mu, \pi \right)}\right]<em>{k,i} = \frac{P\left(Z | X, \mu, \pi \right)^{t}X}{\sum</em>{N}P\left(Z | X, \mu, \pi \right)}$$</p>
<p>In addition to the estimators, the posterior probability obtained from the optimization problem depends on the parameters.</p>
<p>$$P\left(Z | X, \mu, \pi \right) = \left[\frac{\pi_k p\left(x_n|\mu_k \right)}{\sum_{k}\pi_k p\left(x_n|\mu_k \right)} \right]_{n,k}$$</p>
<p>One interesting result from this step is that both of the model parameters depend on the responsibilities $\gamma \left(Z\right)$. This gives the EM algorithm it’s recursive characteristic. First, the model parameters are set randomly. Then the expectation step occurs, in which the responsibilities are calculated. Finally, the expected maximum likelihood parameters are updated given the new responsibilities. This cycle occurs until convergence of the parameters (the parameters are only set randomly once).</p>
<p>The probability of the observations given a particular distribution $k$ can be calculated using matrices the following way:$$P\left(X | \mu \right) = \left[\prod_i \mu_{ki}^{x_ni} \left(1 - \mu_{ki} \right)^{\left(1- x_{ni}\right)} \right]_{n,k} = exp\left(X  \log\left(\mu^{t}\right) + \left(1 - X \right)  \log\left(1 - \mu^{t} \right)\right)$$</p>
<p>Simulation and conclusion</p>
<p>The toy example presented above is implemented in Python. In this case, $10,000$ data points are generated by $K=3$ $Bernoulli\left(\mu_k \right)$ distributions where $\mu_{k} \in \mathbb{R}^{D}, \text{ } D=10$. The results of the fitting process can be seen below. Each colour represents one of the $k$ parameters associated with a specific Bernoulli distribution. Something noticeable at first sight, is that the true parameters and the fitted one’s diverge. However, this is due to the fact that the “yellow” dimension of the fitted resembles the “blue” one from the data. While the “blue” dimension of the fitted resembles the “yellow” one on the true parameters. One thing to remember is that the model never has access to the latent variable $Z$ in the whole fitting process. With this in mind, is clear that the algorithm does well in the sense that the estimated parameters are close to the real ones. Nonetheless, is important to keep in mind that in reality we don’t know the data generator process, which entails that our choice to model the data a certain way under some unknown latent variables will affect greatly the performance of the estimators. The more complex the model, the more information it can capture about the latent variables at the cost of a longer training and less interpretability.</p>
<p>Notes</p>
<p>There is no guarantee that the EM algorithm will end up in a local maximum.</p>
<p>The proof that maximizing the expected log-likelihood of the complete data also improves the log-likelihood of the data given the model parameters can be found in Wikipedia (TLDR: the expected log-likelihood of the complete data is a lower bound of the log-likelihood of the observed data)</p>
<p><img src="../../images/blog/em-algo/fitting.gif" alt="fitting image"></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>style<span style="color:#f92672">.</span>use(<span style="color:#e6db74">&#39;dark_background&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Parameters for the simulation.</span>
</span></span><span style="display:flex;"><span>seed <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(seed)
</span></span><span style="display:flex;"><span>K <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>D <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>N <span style="color:#f92672">=</span> <span style="color:#ae81ff">10000</span>
</span></span><span style="display:flex;"><span>max_it <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>
</span></span><span style="display:flex;"><span>min_change <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Model parameters.</span>
</span></span><span style="display:flex;"><span>true_pi <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">0.1</span>, <span style="color:#ae81ff">0.6</span>, <span style="color:#ae81ff">0.3</span>]])
</span></span><span style="display:flex;"><span>true_mu <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform(size<span style="color:#f92672">=</span>(K, D))
</span></span><span style="display:flex;"><span>X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(shape<span style="color:#f92672">=</span>(N, D))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Populating the data.</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> n <span style="color:#f92672">in</span> range(N):
</span></span><span style="display:flex;"><span>    k <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(<span style="color:#ae81ff">3</span>, p<span style="color:#f92672">=</span>true_pi[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>    X[n, ] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>binomial(n<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> , p<span style="color:#f92672">=</span>true_mu[k, ])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Defining a class for the EM algorithm.</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">EMMBernoulli</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, max_it, min_change, verbose<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;A Class that will hold information related with a expectation
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        maximization algorithm when assuming a mixture of multivariate
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        bernoulli distributions as the data generator process.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Parameters
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        ----------
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        max_it : int
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            Number of maximum iterations to run the simulation.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        min_change : float
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            Minimum change of the log likelihood per iteration to continue
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            the fitting process.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        verbose : float, default True
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            Whether to print or not information about the training.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span> 
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Attributes.</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>X <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>max_it <span style="color:#f92672">=</span> max_it
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>min_change <span style="color:#f92672">=</span> min_change
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>N <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>D <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>verbose <span style="color:#f92672">=</span> verbose
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>K <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pi <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>mu <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>llik <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fit</span>(self, X, K<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Fit the desired data X with a mixture of K 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        multivariate bernoulli distributions using the
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Expected Maximization Algorithm.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Parameters
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        ----------
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        X : np.array
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            Numpy array that will hold the data, it must be filled with ones
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            or zeroes.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        K : int
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            Number of multivariate bernoulli distributions.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Return
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        ------
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        None
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Number of multivariate bernoulli distributions.</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>X <span style="color:#f92672">=</span> X
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>N <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>D <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>K <span style="color:#f92672">=</span> K
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>llik <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Populating randomly the parameters to estimate.</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pi <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> K <span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>K)]])
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>mu <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform(size<span style="color:#f92672">=</span>(self<span style="color:#f92672">.</span>K, self<span style="color:#f92672">.</span>D))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Training loop.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>N):
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">####################</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Expectation step #</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">####################</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># The p_x_mu is a matrix of dim (N, K).</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># This matrix contains the probability of</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># the observation X_{n, k} given mu_{k}</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># P(X_{n, k} | mu_{k}).</span>
</span></span><span style="display:flex;"><span>            p_x_mu <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>exp(self<span style="color:#f92672">.</span>X <span style="color:#f92672">@</span> np<span style="color:#f92672">.</span>log(self<span style="color:#f92672">.</span>mu<span style="color:#f92672">.</span>T) <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>                            (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>X) <span style="color:#f92672">@</span> np<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>mu<span style="color:#f92672">.</span>T))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Calculating the posterior P(Z | X, mu, pi).</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># This matrix is going to be the same shape / dim as p_x_mu.</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># the numerator is equal to:</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># \pi_{k} * p(X_n | \mu_k)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># The denominator is equal to:</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># \sum_k \pi_{k} * p(X_n | \mu_k).</span>
</span></span><span style="display:flex;"><span>            numerator_pos <span style="color:#f92672">=</span> p_x_mu <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>pi
</span></span><span style="display:flex;"><span>            denominator_pos <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(numerator_pos, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, keepdims<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>            posterior <span style="color:#f92672">=</span> numerator_pos <span style="color:#f92672">/</span> denominator_pos
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">##############################</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Log likelihood computation #</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">##############################</span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>llik<span style="color:#f92672">.</span>append(np<span style="color:#f92672">.</span>sum(np<span style="color:#f92672">.</span>log(denominator_pos)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Stopping condition for the fitting.</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> len(self<span style="color:#f92672">.</span>llik) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>                delta <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>llik[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>llik[<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">if</span> delta <span style="color:#f92672">&lt;=</span> self<span style="color:#f92672">.</span>min_change:
</span></span><span style="display:flex;"><span>                    <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Printing the results.</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>verbose:
</span></span><span style="display:flex;"><span>                print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;Iteration: </span><span style="color:#e6db74">{</span>i <span style="color:#e6db74">:</span><span style="color:#e6db74">4d</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> | Log likelihood: </span><span style="color:#e6db74">{</span>self<span style="color:#f92672">.</span>llik[i] <span style="color:#e6db74">:</span><span style="color:#e6db74"> 07.5f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">#####################</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Maximization step #</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">#####################</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Getting the parameters such that</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># it maximizes the current Q(parameters).</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Getting pi(t).</span>
</span></span><span style="display:flex;"><span>            numerator_pi <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(posterior, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, keepdims<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>pi <span style="color:#f92672">=</span>  numerator_pi <span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>N
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Getting mu(t).</span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>mu <span style="color:#f92672">=</span> (posterior<span style="color:#f92672">.</span>T <span style="color:#f92672">@</span> self<span style="color:#f92672">.</span>X) <span style="color:#f92672">/</span> numerator_pi<span style="color:#f92672">.</span>T
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot</span>(self, true_mu, true_pi):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Plot the results of the fitting procedure.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Parameters
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        ----------
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        true_mu : np.array
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            Numpy array containing the true values for mu.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        true_pi : np.array
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            Numpy array containing the true values for pi.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Verifying that the model has been fitted.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> self<span style="color:#f92672">.</span>K <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>, <span style="color:#e6db74">&#34;The model hasn&#39;t being fitted&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Plotting the results.</span>
</span></span><span style="display:flex;"><span>        fig, axs <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">14</span>, <span style="color:#ae81ff">12</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Plotting the mu&#39;s.</span>
</span></span><span style="display:flex;"><span>        axs[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;True mu&#39;s&#34;</span>)
</span></span><span style="display:flex;"><span>        axs[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Fitted mu&#39;s&#34;</span>)
</span></span><span style="display:flex;"><span>        axs[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;True and fitted pi&#39;s&#34;</span>)
</span></span><span style="display:flex;"><span>        axs[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>set_xticks(range(<span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>        axs[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>set_xticklabels((<span style="color:#e6db74">&#34;True&#34;</span>, <span style="color:#e6db74">&#34;Fitted&#34;</span>))
</span></span><span style="display:flex;"><span>        axs[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Log likelihood&#34;</span>)
</span></span><span style="display:flex;"><span>        axs[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#34;Iteration&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>K):
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Calculating the bottom for the stacked bar plot.</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> i <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>                true_bottom <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>                fitted_bottom <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>                pi_bottom <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                true_bottom <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(true_mu[<span style="color:#ae81ff">0</span>:i, ], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>                fitted_bottom <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(self<span style="color:#f92672">.</span>mu[<span style="color:#ae81ff">0</span>:i, ], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>                pi_bottom <span style="color:#f92672">=</span> [np<span style="color:#f92672">.</span>sum(true_pi[<span style="color:#ae81ff">0</span>, :i]), np<span style="color:#f92672">.</span>sum(self<span style="color:#f92672">.</span>pi[<span style="color:#ae81ff">0</span>, :i])]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            color <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>cm<span style="color:#f92672">.</span>viridis((i <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>K)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            axs[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>bar(range(self<span style="color:#f92672">.</span>D),
</span></span><span style="display:flex;"><span>                          true_mu[i, ], 
</span></span><span style="display:flex;"><span>                          bottom<span style="color:#f92672">=</span>true_bottom, 
</span></span><span style="display:flex;"><span>                          color<span style="color:#f92672">=</span>color)
</span></span><span style="display:flex;"><span>            axs[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>bar(range(self<span style="color:#f92672">.</span>D), 
</span></span><span style="display:flex;"><span>                          self<span style="color:#f92672">.</span>mu[i, ], 
</span></span><span style="display:flex;"><span>                          bottom<span style="color:#f92672">=</span>fitted_bottom, 
</span></span><span style="display:flex;"><span>                          color<span style="color:#f92672">=</span>color)
</span></span><span style="display:flex;"><span>            axs[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>bar(range(<span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>                          [true_pi[<span style="color:#ae81ff">0</span>, i], self<span style="color:#f92672">.</span>pi[<span style="color:#ae81ff">0</span>, i]],
</span></span><span style="display:flex;"><span>                          bottom<span style="color:#f92672">=</span>pi_bottom,
</span></span><span style="display:flex;"><span>                          color<span style="color:#f92672">=</span>color)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        axs[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>plot(range(len(self<span style="color:#f92672">.</span>llik)), self<span style="color:#f92672">.</span>llik)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">main</span>():
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> EMMBernoulli(X, max_it, min_change)
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>fit(X, K<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>plot(true_mu, true_pi)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;__main__&#34;</span>:
</span></span><span style="display:flex;"><span>    main()
</span></span></code></pre></div><h1 id="references">references</h1>
<ol>
<li>Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag, Berlin, Heidelberg.</li>
</ol></div>
</div>


    <footer>
    </footer>
</body>
</html>

